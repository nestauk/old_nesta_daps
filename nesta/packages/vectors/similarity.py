from nesta.packages.vectors.read import download_vectors
import faiss


def find_similar_vectors(data, ids, k=20, k_large=1000,
                         n_clusters=250,
                         metric=faiss.METRIC_L1, score_threshold=0.5):
    """Returns a lookup of similar vectors, by ID.
    Similarity is determined by the given metric parameter. For high-dim
    vectors, such as those generated by BERT transformers
    this should be faiss.METRIC_L1. Explicitly, documents with

        (mean(D_large) - D) / mean(D_large) > duplicate_threshold

    are counted as "duplicates" of each other, where D_large is a vector of
    distances of the k_large nearest neighbours, and D is a vector of
    distances of the k nearest neighbours.

    Args:
        data (np.array): An array of vectors.
        ids (np.array): An array of id fields.
        k (int): The maximum number of duplicates that can be found. (default=10)
        k_large (int): The sample size of "background" neighbour documents, for
                       quantifying similarity. The larger this number is, the
                       looser the definition is of "near" duplicates, and
                       so more results will be returned; although it will
                       have no impact on the number of exact duplicates.
                       (default=1000)
        metric (faiss.METRIC*): The distance metric for faiss to use.
                                (default=faiss.METRIC_L2)
        score_threshold (float): See above for definition. (default=0.5)
    """
    n, d = data.shape
    k = n if k > n else k
    k_large = n if k_large > n else k_large
    n_clusters = n if n < n_clusters else n_clusters
    quantizer = faiss.IndexFlat(d, metric)
    index = faiss.IndexIVFFlat(quantizer, d, n_clusters)
    index.train(data)
    index.add(data)
    
    # Make an expansive search to determine the base level of 
    # similarity in this space as the mean similarity of documents
    # in the close vicinity
    index.nprobe = 100
    D, I = index.search(data, k_large)
    base_similarity = D.mean(axis=1)

    # Now subset only the top k results
    D = D[:,:k]
    I = I[:,:k]

    similar_vectors = {}
    for _id, row, sims, base in zip(ids, ids[I], D, base_similarity):
        _id = str(_id)
        scores = (base - sims) / base
        over_threshold = scores > score_threshold
        if over_threshold.sum() <= 1:
            continue
        results = {i: float(s) for i, s in zip(row, scores)
                   if s > score_threshold and _id != i and i not in similar_vectors}
        if len(results) == 0:
            continue
        similar_vectors[_id] = results
    return similar_vectors


def generate_duplicate_links(orm, id_field, database, k=20, k_large=1000,
                             n_clusters=250,
                             metric=faiss.METRIC_L1, duplicate_threshold=0.5,
                             read_chunksize=10000, read_max_chunks=None):
    """Convenience method finding duplicate text via embeddings
    in the database.

    All vectors are read into memory and then indexed with FAISS before
    selecting only very similar vectors as being "duplicates".

    Similarity is determined by the given metric parameter. For high-dim
    vectors, such as those generated by BERT transformers
    this should be faiss.METRIC_L1. Explicitly, documents with

        (mean(D_large) - D) / mean(D_large) > duplicate_threshold

    are counted as "duplicates" of each other, where D_large is a vector of
    distances of the k_large nearest neighbours, and D is a vector of
    distances of the k nearest neighbours.

    Args:
        orm (sqlalchemy.Base): A SqlAlchemy ORM, representing the table of vectors
        id_field (str): The name of the id field in the ORM.
        database (str): The name of the MySQL database ("dev" or "production")
        k (int): The maximum number of duplicates that can be found. (default=10)
        k_large (int): The sample size of "background" neighbour documents, for
                       quantifying similarity. The larger this number is, the
                       looser the definition is of "near" duplicates, and
                       so more results will be returned; although it will
                       have no impact on the number of exact duplicates.
                       (default=1000)
        metric (faiss.METRIC*): The distance metric for faiss to use.
                                (default=faiss.METRIC_L2)
        duplicate_threshold (float): See above for definition. (default=0.9)
        read_chunksize (int): The number of rows to read from the database
                              at a time. You might need to reduce this
                              number if running on WiFi. (default=10000)
        read_max_chunks (int): The maximum number of chunks to
                               read from the database (e.g. if in testing mode).
                               If set to None (default) then all data will be read.
                               (default=None)

    Returns:
        links (json): Rows containing the ids of matching documents
                      and their match weight.
    """
    # Read the data
    data, ids = download_vectors(orm=orm, id_field=id_field, database=database,
                                 chunksize=read_chunksize, 
                                 max_chunks=read_max_chunks)
    # Find all sets of similar vectors
    similar_vectors = find_similar_vectors(data=data, ids=ids, k=k,
                                           k_large=k_large, metric=metric,
                                           n_clusters=n_clusters,
                                           score_threshold=duplicate_threshold)
    # Clean up
    del data
    del ids
    # Structure the output for ingestion to the database as a link table
    links = [{f"{id_field}_1": _id1, f"{id_field}_2": _id2, "weight": weight}
             for _id1, sims in similar_vectors.items()
             for _id2, weight in sims.items()]
    return links
